{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Demonstration of a batched \"DeepSeek/GRPO\" loop with:\n",
        "  - Multiple prompts/tasks per iteration\n",
        "  - Monitoring metrics (avg reward, KL, success rate)\n",
        "  - Periodic checkpoints (saving new_policy)\n",
        "\"\"\"\n",
        "\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoConfig,\n",
        ")\n",
        "import os\n",
        "from typing import List, Tuple\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "class GRPOTrainer:\n",
        "    \"\"\"\n",
        "    \"DeepSeek/GRPO\"-style batched training approach.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name=\"gpt2\",\n",
        "        lr=1e-5,\n",
        "        epsilon=0.2,\n",
        "        kl_coef=0.01,\n",
        "        checkpoint_dir=\"checkpoints\"\n",
        "    ):\n",
        "        \"\"\"\n",
        "        :param model_name: HF model name or local directory\n",
        "        :param lr: learning rate for new_policy\n",
        "        :param epsilon: PPO clip param\n",
        "        :param kl_coef: coefficient for KL penalty\n",
        "        :param checkpoint_dir: where to save model checkpoints\n",
        "        \"\"\"\n",
        "        self.checkpoint_dir = checkpoint_dir\n",
        "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
        "\n",
        "        # Load config & model\n",
        "        self.config = AutoConfig.from_pretrained(model_name)\n",
        "        self.new_policy = AutoModelForCausalLM.from_pretrained(model_name, config=self.config).to(DEVICE)\n",
        "        self.old_policy = copy.deepcopy(self.new_policy).to(DEVICE)\n",
        "        self.old_policy.eval()\n",
        "\n",
        "        # Tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        if self.tokenizer.pad_token_id is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        # Params\n",
        "        self.epsilon = epsilon\n",
        "        self.kl_coef = kl_coef\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(self.new_policy.parameters(), lr=lr)\n",
        "\n",
        "        # We'll use a CrossEntropyLoss manually to compute log-probs\n",
        "        self.ce_loss_fct = nn.CrossEntropyLoss(reduction=\"sum\")\n",
        "\n",
        "    def train_on_batch(\n",
        "        self,\n",
        "        prompts: List[str],\n",
        "        correct_answers: List[str],\n",
        "        group_size=4,\n",
        "        max_new_tokens=20,\n",
        "        temperature=1.0\n",
        "    ) -> dict:\n",
        "        \"\"\"\n",
        "        Perform one batched training step on a list of prompts.\n",
        "        \"\"\"\n",
        "        # 1) Generate from old_policy, gather (prompt, answers, old_logprobs, rewards)\n",
        "        old_samples = []\n",
        "        with self._swap_models_temporarily(self.old_policy):\n",
        "            for prompt, correct_ans in zip(prompts, correct_answers):\n",
        "                answers, logprobs_old = self._generate_answers(\n",
        "                    prompt,\n",
        "                    group_size=group_size,\n",
        "                    max_new_tokens=max_new_tokens,\n",
        "                    temperature=temperature,\n",
        "                    requires_grad=False  # old policy => no gradient\n",
        "                )\n",
        "                rewards = [self._reward_func(a, correct_ans) for a in answers]\n",
        "                old_samples.append((prompt, answers, logprobs_old, rewards))\n",
        "\n",
        "        # 2) Compute new_policy logprobs\n",
        "        all_logprobs_old = []\n",
        "        all_logprobs_new = []\n",
        "        all_rewards = []\n",
        "        total_correct = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        for (prompt, answers, logprobs_old, rewards) in old_samples:\n",
        "            # new policy => needs gradient\n",
        "            _, logprobs_new = self._compute_logprobs(\n",
        "                prompt,\n",
        "                answers,\n",
        "                model=self.new_policy,\n",
        "                requires_grad=True\n",
        "            )\n",
        "\n",
        "            all_logprobs_old.extend(logprobs_old)\n",
        "            all_logprobs_new.extend(logprobs_new.detach().cpu().tolist())\n",
        "            all_rewards.extend(rewards)\n",
        "\n",
        "            for r in rewards:\n",
        "                if r > 0.99:\n",
        "                    total_correct += 1\n",
        "            total_samples += len(rewards)\n",
        "\n",
        "        # Convert lists to Tensors\n",
        "        logprobs_old_t = torch.tensor(all_logprobs_old, device=DEVICE)\n",
        "        logprobs_new_t = torch.tensor(all_logprobs_new, device=DEVICE, requires_grad=True)\n",
        "        rewards_t = torch.tensor(all_rewards, device=DEVICE, dtype=torch.float)\n",
        "\n",
        "        # 3) Compute advantages by group\n",
        "        #    We do chunking in group_size\n",
        "        advantages_list = []\n",
        "        offset = 0\n",
        "        for _ in prompts:\n",
        "            chunk_rewards = rewards_t[offset : offset + group_size]\n",
        "            r_mean = chunk_rewards.mean()\n",
        "            r_std = chunk_rewards.std() + 1e-6\n",
        "            chunk_adv = (chunk_rewards - r_mean) / r_std\n",
        "            advantages_list.append(chunk_adv)\n",
        "            offset += group_size\n",
        "        advantages_t = torch.cat(advantages_list, dim=0)\n",
        "\n",
        "        ratio_t = torch.exp(logprobs_new_t - logprobs_old_t)  # shape [batch_size]\n",
        "        clipped_ratio_t = torch.clamp(ratio_t, 1.0 - self.epsilon, 1.0 + self.epsilon)\n",
        "        obj_min = torch.minimum(ratio_t, clipped_ratio_t) * advantages_t\n",
        "\n",
        "        kl = (logprobs_new_t - logprobs_old_t).mean()\n",
        "        pg_loss = -obj_min.mean() + self.kl_coef * kl\n",
        "\n",
        "        # 4) Gradient update\n",
        "        self.optimizer.zero_grad()\n",
        "        pg_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.new_policy.parameters(), 1.0)\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # 5) refresh old_policy\n",
        "        self.refresh_old_policy()\n",
        "\n",
        "        # metrics\n",
        "        avg_reward = rewards_t.mean().item()\n",
        "        success_rate = float(total_correct) / float(total_samples) if total_samples else 0.0\n",
        "\n",
        "        return {\n",
        "            \"avg_reward\": avg_reward,\n",
        "            \"success_rate\": success_rate,\n",
        "            \"kl_div\": kl.item(),\n",
        "            \"pg_loss\": pg_loss.item(),\n",
        "        }\n",
        "\n",
        "    def refresh_old_policy(self):\n",
        "        self.old_policy.load_state_dict(self.new_policy.state_dict())\n",
        "\n",
        "    def save_checkpoint(self, step: int):\n",
        "        ckpt_path = os.path.join(self.checkpoint_dir, f\"new_policy_step_{step}\")\n",
        "        self.new_policy.save_pretrained(ckpt_path)\n",
        "        print(f\"Checkpoint saved to {ckpt_path}\")\n",
        "\n",
        "    def _generate_answers(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        group_size: int,\n",
        "        max_new_tokens: int,\n",
        "        temperature: float,\n",
        "        requires_grad: bool\n",
        "    ) -> Tuple[List[str], List[float]]:\n",
        "        \"\"\"\n",
        "        Generate 'group_size' completions from the *active* model.\n",
        "        Return (answers, logprobs).\n",
        "        For old policy, we set requires_grad=False.\n",
        "        \"\"\"\n",
        "        # We'll do generation with no_grad to avoid overhead, even if it's new policy\n",
        "        model = self.new_policy\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            input_ids = self.tokenizer(prompt, return_tensors=\"pt\", padding=True).input_ids.to(DEVICE)\n",
        "            attention_mask = self.tokenizer(prompt, return_tensors=\"pt\", padding=True).attention_mask.to(DEVICE)\n",
        "            gen_outputs = model.generate(\n",
        "                input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                do_sample=True,\n",
        "                top_k=0,\n",
        "                temperature=temperature,\n",
        "                num_return_sequences=group_size,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                pad_token_id=self.tokenizer.pad_token_id\n",
        "            )\n",
        "\n",
        "        answers = []\n",
        "        for seq_idx in range(group_size):\n",
        "            seq_ids = gen_outputs[seq_idx]\n",
        "            full_text = self.tokenizer.decode(seq_ids, skip_special_tokens=True)\n",
        "            ans = full_text[len(prompt):].strip()\n",
        "            answers.append(ans)\n",
        "        print(prompt)\n",
        "        print(answers)\n",
        "        # Now compute log-probs with/without grad\n",
        "        _, logprobs_tensor = self._compute_logprobs(\n",
        "            prompt, answers, model, requires_grad=requires_grad\n",
        "        )\n",
        "        print(logprobs_tensor)\n",
        "        print()\n",
        "        return answers, logprobs_tensor.detach().cpu().tolist()\n",
        "\n",
        "    def _compute_logprobs(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        answers: List[str],\n",
        "        model: nn.Module,\n",
        "        requires_grad: bool\n",
        "    ) -> Tuple[List[str], torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Compute log P(answer | prompt) for each answer in a batch.\n",
        "        We'll do this by manually computing cross-entropy from model logits.\n",
        "        If requires_grad=False, wrap in no_grad().\n",
        "        Returns: (answers, logprob_tensor) shape [batch_size].\n",
        "        \"\"\"\n",
        "        if not requires_grad:\n",
        "            with torch.no_grad():\n",
        "                return self._compute_logprobs_impl(prompt, answers, model)\n",
        "        else:\n",
        "            return self._compute_logprobs_impl(prompt, answers, model)\n",
        "\n",
        "    def _compute_logprobs_impl(\n",
        "        self, prompt: str, answers: List[str], model: nn.Module\n",
        "    ) -> Tuple[List[str], torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Actually do the forward pass and compute negative log-likelihood.\n",
        "        We'll sum the cross-entropy over all tokens in [prompt+answer].\n",
        "        \"\"\"\n",
        "        model.eval()\n",
        "        logprob_values = []\n",
        "        for ans in answers:\n",
        "            full_text = prompt + ans\n",
        "            input_ids = self.tokenizer(full_text, return_tensors=\"pt\").input_ids.to(DEVICE)\n",
        "            # Forward pass WITHOUT labels. We'll compute the CE manually:\n",
        "            out = model(input_ids, use_cache=False)\n",
        "            logits = out.logits  # shape [1, seq_len, vocab_size]\n",
        "\n",
        "            # shift so that tokens < t> predict token at t\n",
        "            # next-token-lm: we want p(x_{t+1} | x_{<=t})\n",
        "            shift_logits = logits[:, :-1, :].contiguous()\n",
        "            shift_labels = input_ids[:, 1:].contiguous()\n",
        "\n",
        "            # cross-entropy (sum)\n",
        "            ce_loss = self.ce_loss_fct(\n",
        "                shift_logits.view(-1, shift_logits.size(-1)),\n",
        "                shift_labels.view(-1)\n",
        "            )\n",
        "            # negative log-likelihood:\n",
        "            # (We used reduction='sum', so ce_loss is sum over the tokens)\n",
        "            logprob_values.append(-ce_loss)\n",
        "\n",
        "        # stack shape [batch_size]\n",
        "        logprobs_tensor = torch.stack(logprob_values, dim=0)\n",
        "        return answers, logprobs_tensor\n",
        "\n",
        "    from contextlib import contextmanager\n",
        "    @contextmanager\n",
        "    def _swap_models_temporarily(self, source: nn.Module):\n",
        "        \"\"\"\n",
        "        Temporarily replace self.new_policy params with source's params,\n",
        "        then restore after the context.\n",
        "        \"\"\"\n",
        "        new_state = copy.deepcopy(self.new_policy.state_dict())\n",
        "        self.new_policy.load_state_dict(source.state_dict())\n",
        "        try:\n",
        "            yield\n",
        "        finally:\n",
        "            self.new_policy.load_state_dict(new_state)\n",
        "\n",
        "    def _reward_func(self, candidate: str, correct: str) -> float:\n",
        "        \"\"\"\n",
        "        Simple reward function:\n",
        "          with some corrections for guidance\n",
        "        \"\"\"\n",
        "        tokens = candidate.split(\" \")\n",
        "        reward = 0\n",
        "        if len(tokens) == 1:\n",
        "          reward = .3\n",
        "          try:\n",
        "            tokens[0]\n",
        "            reward += .2\n",
        "          except:\n",
        "            pass\n",
        "          if tokens[0].strip() == correct.strip():\n",
        "            reward = 1\n",
        "        return reward\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Toy data\n",
        "    prompts = [\n",
        "        \"Q: What is 2+3*4?\\nRETURN ONLY THE NUMBER:\",\n",
        "        \"Q: Solve 1+1?\\nRETURN ONLY THE NUMBER:\",\n",
        "        \"Q: What is 10 - 3?\\nRETURN ONLY THE NUMBER:\",\n",
        "    ]\n",
        "    correct_answers = [\"14\", \"2\", \"7\"]\n",
        "\n",
        "    # Hyperparameters\n",
        "    group_size = 4\n",
        "    max_new_tokens = 5\n",
        "    epochs = 5\n",
        "\n",
        "    trainer = GRPOTrainer(\n",
        "        model_name=\"HuggingFaceTB/SmolLM-135M\", # || gpt2\n",
        "        lr=1e-5,\n",
        "        epsilon=0.2,\n",
        "        kl_coef=0.01,\n",
        "        checkpoint_dir=\"checkpoints\"\n",
        "    )\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        metrics = trainer.train_on_batch(\n",
        "            prompts=prompts,\n",
        "            correct_answers=correct_answers,\n",
        "            group_size=group_size,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=1.0\n",
        "        )\n",
        "\n",
        "        print(f\"\\nEpoch {epoch}/{epochs} - Metrics:\")\n",
        "        for k, v in metrics.items():\n",
        "            print(f\"  {k}: {v:.4f}\")\n",
        "\n",
        "        # Save checkpoint every 2 epochs\n",
        "        if epoch % 2 == 0:\n",
        "            trainer.save_checkpoint(step=epoch)\n",
        "\n",
        "    print(\"\\nTraining completed!\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "2gT1LCmYkwtX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "450c08c1-0f16-41f6-e199-3d499c711c68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: What is 2+3*4?\n",
            "RETURN ONLY THE NUMBER:\n",
            "['13 will return', '0, FLO', '- If D is', '10']\n",
            "tensor([-81.9412, -81.3530, -80.2183, -69.6766], device='cuda:0')\n",
            "Q: Solve 1+1?\n",
            "RETURN ONLY THE NUMBER:\n",
            "['TOPIC: Module', '* (MULTIP', 'ALWAYS GUESS', '24. Which']\n",
            "tensor([-96.2953, -88.4987, -89.4476, -88.0674], device='cuda:0')\n",
            "Q: What is 10 - 3?\n",
            "RETURN ONLY THE NUMBER:\n",
            "['10\\nAND', 'And your answer is', 'For text substituting nuclear', '-10 /']\n",
            "tensor([ -78.9078,  -86.0183, -108.9350,  -75.8747], device='cuda:0')\n",
            "\n",
            "Epoch 1/5 - Metrics:\n",
            "  avg_reward: 0.0833\n",
            "  success_rate: 0.0000\n",
            "  kl_div: 0.0000\n",
            "  pg_loss: 0.0000\n",
            "Q: What is 2+3*4?\n",
            "RETURN ONLY THE NUMBER:\n",
            "['(So the diamond', '7\\n3*', '5+ 0', 'CUTVI -']\n",
            "tensor([-89.6841, -77.4595, -82.8370, -94.8110], device='cuda:0')\n",
            "Q: Solve 1+1?\n",
            "RETURN ONLY THE NUMBER:\n",
            "['A successful solution consists=', '-120', 'Express the remainder', 'FIFTH SIDE']\n",
            "tensor([-106.2680,  -80.6138,  -87.1040,  -90.3150], device='cuda:0')\n",
            "Q: What is 10 - 3?\n",
            "RETURN ONLY THE NUMBER:\n",
            "['106 Ã—', 'COUNT OF THAT R', 'MAT: 1', '10-3']\n",
            "tensor([-83.5745, -90.2192, -81.0795, -70.8128], device='cuda:0')\n",
            "\n",
            "Epoch 2/5 - Metrics:\n",
            "  avg_reward: 0.1250\n",
            "  success_rate: 0.0000\n",
            "  kl_div: 0.0000\n",
            "  pg_loss: 0.0000\n",
            "Checkpoint saved to checkpoints/new_policy_step_2\n",
            "Q: What is 2+3*4?\n",
            "RETURN ONLY THE NUMBER:\n",
            "['(k+1', '9998', 'A. 10', '\"23\"']\n",
            "tensor([-78.7317, -78.3086, -76.4481, -76.5966], device='cuda:0')\n",
            "Q: Solve 1+1?\n",
            "RETURN ONLY THE NUMBER:\n",
            "['=\\nSUBST', 'id(s - a', '22 (5', 'OUTERT affordable c']\n",
            "tensor([ -85.5767,  -99.5621,  -87.5875, -112.7432], device='cuda:0')\n",
            "Q: What is 10 - 3?\n",
            "RETURN ONLY THE NUMBER:\n",
            "['10,', 'F: 0gone', 'S: 1', 'Aussie Statistics.']\n",
            "tensor([-72.4533, -95.6437, -79.5778, -95.1205], device='cuda:0')\n",
            "\n",
            "Epoch 3/5 - Metrics:\n",
            "  avg_reward: 0.2083\n",
            "  success_rate: 0.0000\n",
            "  kl_div: 0.0000\n",
            "  pg_loss: 0.0000\n",
            "Q: What is 2+3*4?\n",
            "RETURN ONLY THE NUMBER:\n",
            "['3\\n\\nUsing', 'Question 30', '2 + 3', '7=4*']\n",
            "tensor([-78.6113, -81.0597, -69.7797, -82.1539], device='cuda:0')\n",
            "Q: Solve 1+1?\n",
            "RETURN ONLY THE NUMBER:\n",
            "['Writers Powered by Tree', 'R=RRRRoo', 'you will then find some', 'PROTON']\n",
            "tensor([-109.5782, -100.5881,  -96.4929,  -90.4640], device='cuda:0')\n",
            "Q: What is 10 - 3?\n",
            "RETURN ONLY THE NUMBER:\n",
            "['EASY\\n|', '1010', 'significant digits\\nFIRST', 'He is 1']\n",
            "tensor([-81.9855, -75.4721, -92.5119, -79.1652], device='cuda:0')\n",
            "\n",
            "Epoch 4/5 - Metrics:\n",
            "  avg_reward: 0.2500\n",
            "  success_rate: 0.0000\n",
            "  kl_div: 0.0000\n",
            "  pg_loss: 0.0000\n",
            "Checkpoint saved to checkpoints/new_policy_step_4\n",
            "Q: What is 2+3*4?\n",
            "RETURN ONLY THE NUMBER:\n",
            "['123 AND NOT', 'Who has done', 'Simply put your thoughts as', '--------- PROCPHOD']\n",
            "tensor([-79.2942, -82.6459, -94.0606, -92.8473], device='cuda:0')\n",
            "Q: Solve 1+1?\n",
            "RETURN ONLY THE NUMBER:\n",
            "['The answer is 2', 'PART 1', 'Another way is to', '1\\n10']\n",
            "tensor([-82.2412, -82.1363, -86.6848, -81.5511], device='cuda:0')\n",
            "Q: What is 10 - 3?\n",
            "RETURN ONLY THE NUMBER:\n",
            "['Condition: N will', 'The answer will be', '0. Addition for', 'PROJECTIVE\\nGiven']\n",
            "tensor([-95.1769, -76.8551, -89.9494, -94.8199], device='cuda:0')\n",
            "\n",
            "Epoch 5/5 - Metrics:\n",
            "  avg_reward: 0.0833\n",
            "  success_rate: 0.0000\n",
            "  kl_div: 0.0000\n",
            "  pg_loss: 0.0000\n",
            "\n",
            "Training completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nEOj8l8f3NuR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}