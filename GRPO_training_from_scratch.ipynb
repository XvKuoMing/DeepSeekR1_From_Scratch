{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoConfig,\n",
        ")\n",
        "import os\n",
        "from typing import List, Tuple\n",
        "from contextlib import contextmanager\n",
        "\n",
        "# Set the device to GPU if available, otherwise use CPU\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class GRPOTrainer:\n",
        "    \"\"\"\n",
        "    GRPOTrainer implements a \"DeepSeek/GRPO\"-style batched training approach for language models.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_name=\"gpt2\", lr=1e-5, epsilon=0.2, kl_coef=0.01, checkpoint_dir=\"checkpoints\"):\n",
        "        \"\"\"\n",
        "        Initializes the GRPOTrainer with the specified parameters.\n",
        "\n",
        "        :param model_name: Name of the Hugging Face model to use.\n",
        "        :param lr: Learning rate for the new policy.\n",
        "        :param epsilon: PPO clipping parameter.\n",
        "        :param kl_coef: Coefficient for KL divergence penalty.\n",
        "        :param checkpoint_dir: Directory to save model checkpoints.\n",
        "        \"\"\"\n",
        "        self.checkpoint_dir = checkpoint_dir\n",
        "        os.makedirs(self.checkpoint_dir, exist_ok=True)  # Create checkpoint directory if it doesn't exist\n",
        "\n",
        "        # Load model configuration and model\n",
        "        self.config = AutoConfig.from_pretrained(model_name)\n",
        "        self.new_policy = AutoModelForCausalLM.from_pretrained(model_name, config=self.config).to(DEVICE)\n",
        "        self.old_policy = self.new_policy  # Use the same instance for memory efficiency\n",
        "        self.old_policy.eval()  # Set old policy to evaluation mode\n",
        "\n",
        "        # Initialize tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        if self.tokenizer.pad_token_id is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token  # Set pad token if not defined\n",
        "\n",
        "        # Set parameters\n",
        "        self.epsilon = epsilon\n",
        "        self.kl_coef = kl_coef\n",
        "        self.optimizer = torch.optim.AdamW(self.new_policy.parameters(), lr=lr)  # AdamW optimizer\n",
        "        self.ce_loss_fct = nn.CrossEntropyLoss(reduction=\"sum\")  # Cross-entropy loss for log probabilities\n",
        "\n",
        "    def train_on_batch(self, prompts: List[str], correct_answers: List[str], group_size=4, max_new_tokens=20, temperature=1.0) -> dict:\n",
        "        \"\"\"\n",
        "        Perform one batched training step on a list of prompts.\n",
        "\n",
        "        :param prompts: List of input prompts.\n",
        "        :param correct_answers: List of correct answers corresponding to the prompts.\n",
        "        :param group_size: Number of answers to generate for each prompt.\n",
        "        :param max_new_tokens: Maximum number of new tokens to generate.\n",
        "        :param temperature: Sampling temperature for generation.\n",
        "        :return: A dictionary containing training metrics.\n",
        "        \"\"\"\n",
        "        old_samples = []  # Store samples generated from the old policy\n",
        "        with self._swap_models_temporarily(self.old_policy):  # Temporarily swap models\n",
        "            for prompt, correct_ans in zip(prompts, correct_answers):\n",
        "                # Generate answers using the old policy\n",
        "                answers, logprobs_old = self._generate_answers(prompt, group_size, max_new_tokens, temperature, requires_grad=False)\n",
        "                rewards = [self._reward_func(a, correct_ans) for a in answers]  # Calculate rewards\n",
        "                old_samples.append((prompt, answers, logprobs_old, rewards))  # Store results\n",
        "\n",
        "        # Initialize lists for log probabilities and rewards\n",
        "        all_logprobs_old, all_logprobs_new, all_rewards = [], [], []\n",
        "        total_correct, total_samples = 0, 0  # Initialize counters for correct answers and total samples\n",
        "\n",
        "        for (prompt, answers, logprobs_old, rewards) in old_samples:\n",
        "            # Compute new log probabilities using the new policy\n",
        "            _, logprobs_new = self._compute_logprobs(prompt, answers, model=self.new_policy, requires_grad=True)\n",
        "\n",
        "            all_logprobs_old.extend(logprobs_old)  # Collect old log probabilities\n",
        "            all_logprobs_new.extend(logprobs_new.detach().cpu().tolist())  # Collect new log probabilities\n",
        "            all_rewards.extend(rewards)  # Collect rewards\n",
        "\n",
        "            total_correct += sum(r > 0.99 for r in rewards)  # Count correct rewards\n",
        "            total_samples += len(rewards)  # Update total samples\n",
        "\n",
        "        # Convert lists to tensors for computation\n",
        "        logprobs_old_t = torch.tensor(all_logprobs_old, device=DEVICE)\n",
        "        logprobs_new_t = torch.tensor(all_logprobs_new, device=DEVICE, requires_grad=True)\n",
        "        rewards_t = torch.tensor(all_rewards, device=DEVICE, dtype=torch.float)\n",
        "\n",
        "        # Compute advantages for the current batch\n",
        "        advantages_t = self._compute_advantages(rewards_t, group_size, prompts)\n",
        "\n",
        "        # Calculate the ratio of new to old log probabilities\n",
        "        ratio_t = torch.exp(logprobs_new_t - logprobs_old_t)\n",
        "        clipped_ratio_t = torch.clamp(ratio_t, 1.0 - self.epsilon, 1.0 + self.epsilon)  # Clip the ratio\n",
        "        obj_min = torch.minimum(ratio_t, clipped_ratio_t) * advantages_t  # Objective function\n",
        "\n",
        "        kl = (logprobs_new_t - logprobs_old_t).mean()  # Compute KL divergence\n",
        "        pg_loss = -obj_min.mean() + self.kl_coef * kl  # Calculate policy gradient loss\n",
        "\n",
        "        self.optimizer.zero_grad()  # Reset gradients\n",
        "        pg_loss.backward()  # Backpropagate loss\n",
        "        torch.nn.utils.clip_grad_norm_(self.new_policy.parameters(), 1.0)  # Clip gradients\n",
        "        self.optimizer.step()  # Update parameters\n",
        "\n",
        "        self.refresh_old_policy()  # Refresh old policy parameters\n",
        "\n",
        "        avg_reward = rewards_t.mean().item()  # Calculate average reward\n",
        "        success_rate = float(total_correct) / float(total_samples) if total_samples else 0.0  # Calculate success rate\n",
        "\n",
        "        return {\n",
        "            \"avg_reward\": avg_reward,\n",
        "            \"success_rate\": success_rate,\n",
        "            \"kl_div\": kl.item(),\n",
        "            \"pg_loss\": pg_loss.item(),\n",
        "        }\n",
        "\n",
        "    def _compute_advantages(self, rewards_t, group_size, prompts):\n",
        "        \"\"\"\n",
        "        Compute advantages for the current batch based on rewards.\n",
        "\n",
        "        :param rewards_t: Tensor of rewards for the current batch.\n",
        "        :param group_size: Number of answers generated for each prompt.\n",
        "        :param prompts: List of input prompts.\n",
        "        :return: Tensor of advantages.\n",
        "        \"\"\"\n",
        "        advantages_list = []  # List to store advantages for each prompt\n",
        "        offset = 0  # Initialize offset for slicing rewards\n",
        "\n",
        "        for _ in prompts:\n",
        "            chunk_rewards = rewards_t[offset: offset + group_size]  # Get rewards for the current prompt\n",
        "            r_mean = chunk_rewards.mean()  # Calculate mean reward\n",
        "            r_std = chunk_rewards.std() + 1e-6  # Calculate standard deviation with a small epsilon to avoid division by zero\n",
        "            chunk_adv = (chunk_rewards - r_mean) / r_std  # Normalize advantages\n",
        "            advantages_list.append(chunk_adv)  # Append to advantages list\n",
        "            offset += group_size  # Update offset for the next prompt\n",
        "\n",
        "        return torch.cat(advantages_list, dim=0)  # Concatenate advantages into a single tensor\n",
        "\n",
        "    def refresh_old_policy(self):\n",
        "        \"\"\"\n",
        "        Refresh the old policy parameters by copying the new policy parameters.\n",
        "        This is done after each training step to keep the old policy updated.\n",
        "        \"\"\"\n",
        "        for old_param, new_param in zip(self.old_policy.parameters(), self.new_policy.parameters()):\n",
        "            old_param.data.copy_(new_param.data)  # Copy new policy parameters to old policy\n",
        "\n",
        "    def save_checkpoint(self, step: int):\n",
        "        \"\"\"\n",
        "        Save the current model and optimizer state to a checkpoint.\n",
        "\n",
        "        :param step: Current training step for naming the checkpoint.\n",
        "        \"\"\"\n",
        "        ckpt_path = os.path.join(self.checkpoint_dir, f\"new_policy_step_{step}\")\n",
        "        self.new_policy.save_pretrained(ckpt_path)  # Save model weights\n",
        "        torch.save(self.optimizer.state_dict(), os.path.join(ckpt_path, \"optimizer.pt\"))  # Save optimizer state\n",
        "        print(f\"Checkpoint saved to {ckpt_path}\")\n",
        "\n",
        "    def _generate_answers(self, prompt: str, group_size: int, max_new_tokens: int, temperature: float, requires_grad: bool) -> Tuple[List[str], List[float]]:\n",
        "        \"\"\"\n",
        "        Generate answers for a given prompt using the current policy.\n",
        "\n",
        "        :param prompt: Input prompt for which to generate answers.\n",
        "        :param group_size: Number of answers to generate.\n",
        "        :param max_new_tokens: Maximum number of new tokens to generate.\n",
        "        :param temperature: Sampling temperature for generation.\n",
        "        :param requires_grad: Whether to compute gradients for the generated answers.\n",
        "        :return: A tuple containing the generated answers and their log probabilities.\n",
        "        \"\"\"\n",
        "        model = self.new_policy\n",
        "        model.eval()  # Set model to evaluation mode\n",
        "\n",
        "        with torch.no_grad():  # Disable gradient computation\n",
        "            input_ids = self.tokenizer(prompt, return_tensors=\"pt\", padding=True).input_ids.to(DEVICE)  # Tokenize prompt\n",
        "            gen_outputs = model.generate(\n",
        "                input_ids,\n",
        "                do_sample=True,\n",
        "                top_k=0,\n",
        "                temperature=temperature,\n",
        "                num_return_sequences=group_size,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                pad_token_id=self.tokenizer.pad_token_id  # Use pad token for padding\n",
        "            )\n",
        "\n",
        "        answers = []  # List to store generated answers\n",
        "        for seq_idx in range(group_size):\n",
        "            seq_ids = gen_outputs[seq_idx]  # Get the generated sequence\n",
        "            full_text = self.tokenizer.decode(seq_ids, skip_special_tokens=True)  # Decode the sequence to text\n",
        "            ans = full_text[len(prompt):].strip()  # Extract the answer by removing the prompt\n",
        "            answers.append(ans)  # Append the answer to the list\n",
        "\n",
        "        # Compute log probabilities for the generated answers\n",
        "        _, logprobs_tensor = self._compute_logprobs(prompt, answers, model, requires_grad=requires_grad)\n",
        "        return answers, logprobs_tensor.detach().cpu().tolist()  # Return answers and log probabilities\n",
        "\n",
        "    def _compute_logprobs(self, prompt: str, answers: List[str], model: nn.Module, requires_grad: bool) -> Tuple[List[str], torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Compute log probabilities for the generated answers.\n",
        "\n",
        "        :param prompt: Input prompt.\n",
        "        :param answers: List of generated answers.\n",
        "        :param model: The model to use for computing log probabilities.\n",
        "        :param requires_grad: Whether to compute gradients.\n",
        "        :return: A tuple containing the answers and their log probabilities.\n",
        "        \"\"\"\n",
        "        if not requires_grad:\n",
        "            with torch.no_grad():  # Disable gradient computation\n",
        "                return self._compute_logprobs_impl(prompt, answers, model)\n",
        "        else:\n",
        "            return self._compute_logprobs_impl(prompt, answers, model)\n",
        "\n",
        "    def _compute_logprobs_impl(self, prompt: str, answers: List[str], model: nn.Module) -> Tuple[List[str], torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Implementation of log probability computation.\n",
        "\n",
        "        :param prompt: Input prompt.\n",
        "        :param answers: List of generated answers.\n",
        "        :param model: The model to use for computing log probabilities.\n",
        "        :return: A tuple containing the answers and their log probabilities.\n",
        "        \"\"\"\n",
        "        model.eval()  # Set model to evaluation mode\n",
        "        logprob_values = []  # List to store log probabilities\n",
        "        prompt_ids = self.tokenizer(prompt, return_tensors=\"pt\").input_ids.to(DEVICE)  # Tokenize prompt\n",
        "        prompt_length = prompt_ids.shape[1]  # Get the length of the prompt\n",
        "\n",
        "        # Prepare full texts for log probability computation\n",
        "        full_texts = [prompt + ans for ans in answers]\n",
        "        inputs = self.tokenizer(full_texts, return_tensors=\"pt\", padding=True, truncation=True).to(DEVICE)  # Tokenize full texts\n",
        "\n",
        "        with torch.no_grad():  # Disable gradient computation\n",
        "            outputs = model(**inputs)  # Forward pass through the model\n",
        "\n",
        "        # Compute log probabilities for each answer\n",
        "        for i in range(len(answers)):\n",
        "            answer_start = prompt_length - 1  # Start index for the answer\n",
        "            answer_end = inputs.input_ids[i].ne(self.tokenizer.pad_token_id).sum() - 1  # End index for the answer\n",
        "\n",
        "            if answer_end <= answer_start:  # Check for valid answer range\n",
        "                logprob_values.append(torch.tensor(0.0, device=DEVICE))  # Append zero log probability if invalid\n",
        "                continue\n",
        "\n",
        "            logits = outputs.logits[i, answer_start:answer_end]  # Get logits for the answer\n",
        "            labels = inputs.input_ids[i, answer_start + 1:answer_end + 1]  # Get labels for the answer\n",
        "            ce_loss = self.ce_loss_fct(logits, labels)  # Compute cross-entropy loss\n",
        "            logprob_values.append(-ce_loss)  # Append negative loss as log probability\n",
        "        print(prompt)\n",
        "        print(answers)\n",
        "        print(logprob_values)\n",
        "        return answers, torch.tensor(logprob_values, device=DEVICE)  # Return answers and log probabilities as tensor\n",
        "\n",
        "    @contextmanager\n",
        "    def _swap_models_temporarily(self, source: nn.Module):\n",
        "        \"\"\"\n",
        "        Temporarily replace self.new_policy params with source's params,\n",
        "        then restore after the context.\n",
        "\n",
        "        :param source: The model whose parameters will be swapped in.\n",
        "        \"\"\"\n",
        "        # Save the current state of the new_policy\n",
        "        new_state = {name: param.data.clone() for name, param in self.new_policy.named_parameters()}\n",
        "\n",
        "        # Load only the parameters that exist in the new_policy\n",
        "        source_state = source.state_dict()  # Get state dict of the source model\n",
        "        for name, param in self.new_policy.named_parameters():\n",
        "            if name in source_state:  # Check if parameter exists in source\n",
        "                param.data.copy_(source_state[name])  # Copy parameter data\n",
        "\n",
        "        try:\n",
        "            yield  # Yield control back to the context\n",
        "        finally:\n",
        "            # Restore the original parameters\n",
        "            for name, param in self.new_policy.named_parameters():\n",
        "                if name in new_state:  # Check if parameter exists in saved state\n",
        "                    param.data.copy_(new_state[name])  # Restore parameter data\n",
        "\n",
        "    def _reward_func(self, candidate: str, correct: str) -> float:\n",
        "        \"\"\"\n",
        "        Calculate the reward based on the candidate answer and the correct answer.\n",
        "        If the candidate is a number, it checks if it is close to the correct answer.\n",
        "        Otherwise, it checks for string equality.\n",
        "\n",
        "        :param candidate: The generated answer from the model.\n",
        "        :param correct: The correct answer to compare against.\n",
        "        :return: A float representing the reward (1.0 for correct, 0.0 for incorrect).\n",
        "        \"\"\"\n",
        "        candidate = candidate.split(\"\\n\")[0] # Removing thinking process\n",
        "        candidate = candidate.strip()  # Remove leading/trailing whitespace\n",
        "        correct = correct.strip()  # Remove leading/trailing whitespace\n",
        "\n",
        "        if candidate == correct:\n",
        "          return 1.0\n",
        "\n",
        "        reward = .0\n",
        "\n",
        "        if len(candidate.split(\" \")) > 1:\n",
        "          reward += -.3\n",
        "        else:\n",
        "          reward += -.1\n",
        "\n",
        "        try:\n",
        "          candidate_num = float(candidate)\n",
        "          reward += .5\n",
        "        except ValueError:\n",
        "          reward += -.3 # Penalty\n",
        "\n",
        "        return reward\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to run the training process.\n",
        "    It initializes the trainer and runs multiple epochs of training.\n",
        "    \"\"\"\n",
        "    prompts = [\n",
        "        \"Q: What is 2+3*4?\\nRETURN ONLY THE NUMBER:\",\n",
        "        \"Q: Solve 1+1?\\nRETURN ONLY THE NUMBER:\",\n",
        "        \"Q: What is 10 - 3?\\nRETURN ONLY THE NUMBER:\",\n",
        "    ]\n",
        "    correct_answers = [\"14\", \"2\", \"7\"]  # Expected answers for the prompts\n",
        "\n",
        "    group_size = 4  # Number of answers to generate for each prompt\n",
        "    max_new_tokens = 5  # Maximum number of new tokens to generate\n",
        "    epochs = 5  # Number of training epochs\n",
        "\n",
        "    # Initialize the GRPOTrainer with specified parameters\n",
        "    trainer = GRPOTrainer(\n",
        "        model_name=\"prithivMLmods/Bellatrix-Tiny-1B-v2\",  # Model name to load\n",
        "        lr=1e-5,  # Learning rate\n",
        "        epsilon=0.2,  # PPO clipping parameter\n",
        "        kl_coef=0.01,  # KL divergence coefficient\n",
        "        checkpoint_dir=\"checkpoints\"  # Directory for saving checkpoints\n",
        "    )\n",
        "\n",
        "    # Run training for the specified number of epochs\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        metrics = trainer.train_on_batch(\n",
        "            prompts=prompts,\n",
        "            correct_answers=correct_answers,\n",
        "            group_size=group_size,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=.5  # Temperature for sampling\n",
        "        )\n",
        "\n",
        "        # Print metrics for the current epoch\n",
        "        print(f\"\\nEpoch {epoch}/{epochs} - Metrics:\")\n",
        "        for k, v in metrics.items():\n",
        "            print(f\"  {k}: {v:.4f}\")  # Display each metric\n",
        "\n",
        "        # Save checkpoint every 2 epochs\n",
        "        if epoch % 2 == 0:\n",
        "            trainer.save_checkpoint(step=epoch)  # Save model checkpoint\n",
        "\n",
        "    print(\"\\nTraining completed!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()  # Execute the main function"
      ],
      "metadata": {
        "id": "2gT1LCmYkwtX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dedba7b7-1a79-4a5b-ed93-7dc8c0349ec9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: What is 2+3*4?\n",
            "RETURN ONLY THE NUMBER:\n",
            "['2+3*', \"11\\n\\nLet's\", \"14\\n\\nLet's\", \"11\\n\\nLet's\"]\n",
            "[tensor(-7.5274, device='cuda:0'), tensor(-10.4278, device='cuda:0'), tensor(-9.8115, device='cuda:0'), tensor(-10.4278, device='cuda:0')]\n",
            "Q: Solve 1+1?\n",
            "RETURN ONLY THE NUMBER:\n",
            "['2\\n\\n**Explanation', \"2\\n\\nLet's\", '2\\n\\nQ:', '2\\n\\n**Solution']\n",
            "[tensor(-11.1349, device='cuda:0'), tensor(-8.9757, device='cuda:0'), tensor(-10.0975, device='cuda:0'), tensor(-10.7723, device='cuda:0')]\n",
            "Q: What is 10 - 3?\n",
            "RETURN ONLY THE NUMBER:\n",
            "[\"7\\n\\nLet's\", \"7\\n\\nLet's\", \"7\\n\\nLet's\", \"7\\n\\nLet's\"]\n",
            "[tensor(-9.5574, device='cuda:0'), tensor(-9.5574, device='cuda:0'), tensor(-9.5574, device='cuda:0'), tensor(-9.5574, device='cuda:0')]\n",
            "Q: What is 2+3*4?\n",
            "RETURN ONLY THE NUMBER:\n",
            "['2+3*', \"11\\n\\nLet's\", \"14\\n\\nLet's\", \"11\\n\\nLet's\"]\n",
            "[tensor(-7.5274, device='cuda:0'), tensor(-10.4278, device='cuda:0'), tensor(-9.8115, device='cuda:0'), tensor(-10.4278, device='cuda:0')]\n",
            "Q: Solve 1+1?\n",
            "RETURN ONLY THE NUMBER:\n",
            "['2\\n\\n**Explanation', \"2\\n\\nLet's\", '2\\n\\nQ:', '2\\n\\n**Solution']\n",
            "[tensor(-11.1349, device='cuda:0'), tensor(-8.9757, device='cuda:0'), tensor(-10.0975, device='cuda:0'), tensor(-10.7723, device='cuda:0')]\n",
            "Q: What is 10 - 3?\n",
            "RETURN ONLY THE NUMBER:\n",
            "[\"7\\n\\nLet's\", \"7\\n\\nLet's\", \"7\\n\\nLet's\", \"7\\n\\nLet's\"]\n",
            "[tensor(-9.5574, device='cuda:0'), tensor(-9.5574, device='cuda:0'), tensor(-9.5574, device='cuda:0'), tensor(-9.5574, device='cuda:0')]\n",
            "\n",
            "Epoch 1/5 - Metrics:\n",
            "  avg_reward: 0.7833\n",
            "  success_rate: 0.7500\n",
            "  kl_div: 0.0000\n",
            "  pg_loss: 0.0000\n",
            "Q: What is 2+3*4?\n",
            "RETURN ONLY THE NUMBER:\n",
            "[\"14\\n\\nLet's\", \"11\\n\\nLet's\", \"11\\n\\nLet's\", \"11\\n\\nLet's\"]\n",
            "[tensor(-9.8115, device='cuda:0'), tensor(-10.4278, device='cuda:0'), tensor(-10.4278, device='cuda:0'), tensor(-10.4278, device='cuda:0')]\n",
            "Q: Solve 1+1?\n",
            "RETURN ONLY THE NUMBER:\n",
            "['2\\nQ:', '2\\n\\n**Solution', '2\\n\\nQ:', '2\\n\\n**Step']\n",
            "[tensor(-10.4618, device='cuda:0'), tensor(-10.7723, device='cuda:0'), tensor(-10.0975, device='cuda:0'), tensor(-10.3406, device='cuda:0')]\n",
            "Q: What is 10 - 3?\n",
            "RETURN ONLY THE NUMBER:\n",
            "[\"7\\n\\nLet's\", \"7\\n\\nLet's\", \"7\\n\\nLet's\", \"7\\n\\nLet's\"]\n",
            "[tensor(-9.5574, device='cuda:0'), tensor(-9.5574, device='cuda:0'), tensor(-9.5574, device='cuda:0'), tensor(-9.5574, device='cuda:0')]\n",
            "Q: What is 2+3*4?\n",
            "RETURN ONLY THE NUMBER:\n",
            "[\"14\\n\\nLet's\", \"11\\n\\nLet's\", \"11\\n\\nLet's\", \"11\\n\\nLet's\"]\n",
            "[tensor(-9.8115, device='cuda:0'), tensor(-10.4278, device='cuda:0'), tensor(-10.4278, device='cuda:0'), tensor(-10.4278, device='cuda:0')]\n",
            "Q: Solve 1+1?\n",
            "RETURN ONLY THE NUMBER:\n",
            "['2\\nQ:', '2\\n\\n**Solution', '2\\n\\nQ:', '2\\n\\n**Step']\n",
            "[tensor(-10.4618, device='cuda:0'), tensor(-10.7723, device='cuda:0'), tensor(-10.0975, device='cuda:0'), tensor(-10.3406, device='cuda:0')]\n",
            "Q: What is 10 - 3?\n",
            "RETURN ONLY THE NUMBER:\n",
            "[\"7\\n\\nLet's\", \"7\\n\\nLet's\", \"7\\n\\nLet's\", \"7\\n\\nLet's\"]\n",
            "[tensor(-9.5574, device='cuda:0'), tensor(-9.5574, device='cuda:0'), tensor(-9.5574, device='cuda:0'), tensor(-9.5574, device='cuda:0')]\n",
            "\n",
            "Epoch 2/5 - Metrics:\n",
            "  avg_reward: 0.8500\n",
            "  success_rate: 0.7500\n",
            "  kl_div: 0.0000\n",
            "  pg_loss: 0.0000\n",
            "Checkpoint saved to checkpoints/new_policy_step_2\n",
            "Q: What is 2+3*4?\n",
            "RETURN ONLY THE NUMBER:\n",
            "[\"19\\n\\nLet's\", \"14\\n\\nLet's\", \"14\\n\\nLet's\", '2 + 3']\n",
            "[tensor(-9.8582, device='cuda:0'), tensor(-9.8115, device='cuda:0'), tensor(-9.8115, device='cuda:0'), tensor(-10.0593, device='cuda:0')]\n",
            "Q: Solve 1+1?\n",
            "RETURN ONLY THE NUMBER:\n",
            "['2\\nQ:', '2\\n\\nQ:', '2\\n\\nA:', '2\\n\\n**Step']\n",
            "[tensor(-10.4618, device='cuda:0'), tensor(-10.0975, device='cuda:0'), tensor(-9.9706, device='cuda:0'), tensor(-10.3406, device='cuda:0')]\n",
            "Q: What is 10 - 3?\n",
            "RETURN ONLY THE NUMBER:\n",
            "[\"7\\n\\nLet's\", \"7\\n\\nLet's\", \"7\\n\\nLet's\", \"7\\n\\nLet's\"]\n",
            "[tensor(-9.5574, device='cuda:0'), tensor(-9.5574, device='cuda:0'), tensor(-9.5574, device='cuda:0'), tensor(-9.5574, device='cuda:0')]\n",
            "Q: What is 2+3*4?\n",
            "RETURN ONLY THE NUMBER:\n",
            "[\"19\\n\\nLet's\", \"14\\n\\nLet's\", \"14\\n\\nLet's\", '2 + 3']\n",
            "[tensor(-9.8582, device='cuda:0'), tensor(-9.8115, device='cuda:0'), tensor(-9.8115, device='cuda:0'), tensor(-10.0593, device='cuda:0')]\n",
            "Q: Solve 1+1?\n",
            "RETURN ONLY THE NUMBER:\n",
            "['2\\nQ:', '2\\n\\nQ:', '2\\n\\nA:', '2\\n\\n**Step']\n",
            "[tensor(-10.4618, device='cuda:0'), tensor(-10.0975, device='cuda:0'), tensor(-9.9706, device='cuda:0'), tensor(-10.3406, device='cuda:0')]\n",
            "Q: What is 10 - 3?\n",
            "RETURN ONLY THE NUMBER:\n",
            "[\"7\\n\\nLet's\", \"7\\n\\nLet's\", \"7\\n\\nLet's\", \"7\\n\\nLet's\"]\n",
            "[tensor(-9.5574, device='cuda:0'), tensor(-9.5574, device='cuda:0'), tensor(-9.5574, device='cuda:0'), tensor(-9.5574, device='cuda:0')]\n",
            "\n",
            "Epoch 3/5 - Metrics:\n",
            "  avg_reward: 0.8167\n",
            "  success_rate: 0.8333\n",
            "  kl_div: 0.0000\n",
            "  pg_loss: -0.0000\n",
            "Q: What is 2+3*4?\n",
            "RETURN ONLY THE NUMBER:\n",
            "[\"11\\n\\nLet's\", \"13\\n\\nLet's\", \"11\\n\\nLet's\", \"11\\n\\nLet's\"]\n",
            "[tensor(-10.4278, device='cuda:0'), tensor(-10.2938, device='cuda:0'), tensor(-10.4278, device='cuda:0'), tensor(-10.4278, device='cuda:0')]\n",
            "Q: Solve 1+1?\n",
            "RETURN ONLY THE NUMBER:\n",
            "['2\\n\\n**Solution', '2\\n\\n**Step', '2\\n\\nA:', '2\\n\\nA:']\n",
            "[tensor(-10.7723, device='cuda:0'), tensor(-10.3406, device='cuda:0'), tensor(-9.9706, device='cuda:0'), tensor(-9.9706, device='cuda:0')]\n",
            "Q: What is 10 - 3?\n",
            "RETURN ONLY THE NUMBER:\n",
            "[\"7\\n\\nLet's\", \"7\\n\\nLet's\", \"7\\n\\nLet's\", \"7\\n\\nLet's\"]\n",
            "[tensor(-9.5574, device='cuda:0'), tensor(-9.5574, device='cuda:0'), tensor(-9.5574, device='cuda:0'), tensor(-9.5574, device='cuda:0')]\n",
            "Q: What is 2+3*4?\n",
            "RETURN ONLY THE NUMBER:\n",
            "[\"11\\n\\nLet's\", \"13\\n\\nLet's\", \"11\\n\\nLet's\", \"11\\n\\nLet's\"]\n",
            "[tensor(-10.4278, device='cuda:0'), tensor(-10.2938, device='cuda:0'), tensor(-10.4278, device='cuda:0'), tensor(-10.4278, device='cuda:0')]\n",
            "Q: Solve 1+1?\n",
            "RETURN ONLY THE NUMBER:\n",
            "['2\\n\\n**Solution', '2\\n\\n**Step', '2\\n\\nA:', '2\\n\\nA:']\n",
            "[tensor(-10.7723, device='cuda:0'), tensor(-10.3406, device='cuda:0'), tensor(-9.9706, device='cuda:0'), tensor(-9.9706, device='cuda:0')]\n",
            "Q: What is 10 - 3?\n",
            "RETURN ONLY THE NUMBER:\n",
            "[\"7\\n\\nLet's\", \"7\\n\\nLet's\", \"7\\n\\nLet's\", \"7\\n\\nLet's\"]\n",
            "[tensor(-9.5574, device='cuda:0'), tensor(-9.5574, device='cuda:0'), tensor(-9.5574, device='cuda:0'), tensor(-9.5574, device='cuda:0')]\n",
            "\n",
            "Epoch 4/5 - Metrics:\n",
            "  avg_reward: 0.8000\n",
            "  success_rate: 0.6667\n",
            "  kl_div: 0.0000\n",
            "  pg_loss: 0.0000\n",
            "Checkpoint saved to checkpoints/new_policy_step_4\n",
            "Q: What is 2+3*4?\n",
            "RETURN ONLY THE NUMBER:\n",
            "[\"11\\n\\nLet's\", \"13\\n\\nLet's\", \"11\\n\\nLet's\", \"14\\n\\nLet's\"]\n",
            "[tensor(-10.4278, device='cuda:0'), tensor(-10.2938, device='cuda:0'), tensor(-10.4278, device='cuda:0'), tensor(-9.8115, device='cuda:0')]\n",
            "Q: Solve 1+1?\n",
            "RETURN ONLY THE NUMBER:\n",
            "['2\\nA:', '2\\n\\nQ:', '2\\n\\n**Step', \"2\\n\\nLet's\"]\n",
            "[tensor(-10.0226, device='cuda:0'), tensor(-10.0975, device='cuda:0'), tensor(-10.3406, device='cuda:0'), tensor(-8.9757, device='cuda:0')]\n",
            "Q: What is 10 - 3?\n",
            "RETURN ONLY THE NUMBER:\n",
            "[\"7\\n\\nLet's\", \"7\\n\\nLet's\", \"7\\n\\nLet's\", \"7\\n\\nLet's\"]\n",
            "[tensor(-9.5574, device='cuda:0'), tensor(-9.5574, device='cuda:0'), tensor(-9.5574, device='cuda:0'), tensor(-9.5574, device='cuda:0')]\n",
            "Q: What is 2+3*4?\n",
            "RETURN ONLY THE NUMBER:\n",
            "[\"11\\n\\nLet's\", \"13\\n\\nLet's\", \"11\\n\\nLet's\", \"14\\n\\nLet's\"]\n",
            "[tensor(-10.4278, device='cuda:0'), tensor(-10.2938, device='cuda:0'), tensor(-10.4278, device='cuda:0'), tensor(-9.8115, device='cuda:0')]\n",
            "Q: Solve 1+1?\n",
            "RETURN ONLY THE NUMBER:\n",
            "['2\\nA:', '2\\n\\nQ:', '2\\n\\n**Step', \"2\\n\\nLet's\"]\n",
            "[tensor(-10.0226, device='cuda:0'), tensor(-10.0975, device='cuda:0'), tensor(-10.3406, device='cuda:0'), tensor(-8.9757, device='cuda:0')]\n",
            "Q: What is 10 - 3?\n",
            "RETURN ONLY THE NUMBER:\n",
            "[\"7\\n\\nLet's\", \"7\\n\\nLet's\", \"7\\n\\nLet's\", \"7\\n\\nLet's\"]\n",
            "[tensor(-9.5574, device='cuda:0'), tensor(-9.5574, device='cuda:0'), tensor(-9.5574, device='cuda:0'), tensor(-9.5574, device='cuda:0')]\n",
            "\n",
            "Epoch 5/5 - Metrics:\n",
            "  avg_reward: 0.8500\n",
            "  success_rate: 0.7500\n",
            "  kl_div: 0.0000\n",
            "  pg_loss: 0.0000\n",
            "\n",
            "Training completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nEOj8l8f3NuR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}